{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.agent import Agent\n",
    "from phi.model.openai import OpenAIChat\n",
    "from phi.tools.duckduckgo import DuckDuckGo\n",
    "\n",
    "web_agent = Agent(\n",
    "    name=\"Web Agent\",\n",
    "    model=OpenAIChat(id=\"gpt-4o\",api_key='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'),\n",
    "    tools=[DuckDuckGo()],\n",
    "    instructions=[\"Always include sources\"],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd54541b030e49d8928086e916035e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "web_agent.print_response(\"Tell me about OpenAI Sora?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Agent.run of Agent(model=OpenAIChat(id='gpt-4o', name='OpenAIChat', provider='OpenAI', metrics={'response_times': [1.2400782000040635, 3.000474900007248], 'time_to_first_token': [1.2255194999743253, 1.5130166000453755], 'input_tokens': 1564, 'output_tokens': 141, 'prompt_tokens': 1564, 'completion_tokens': 141, 'total_tokens': 1705, 'tool_call_times': {'duckduckgo_search': [2.1530698999995366]}}, response_format=None, tools=[{'type': 'function', 'function': {'name': 'duckduckgo_search', 'description': 'Use this function to search DuckDuckGo for a query.\\n\\nArgs:\\n    query(str): The query to search for.\\n    max_results (optional, default=5): The maximum number of results to return.\\n\\nReturns:\\n    The result from DuckDuckGo.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string'}, 'max_results': {'type': 'number'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'duckduckgo_news', 'description': 'Use this function to get the latest news from DuckDuckGo.\\n\\nArgs:\\n    query(str): The query to search for.\\n    max_results (optional, default=5): The maximum number of results to return.\\n\\nReturns:\\n    The latest news from DuckDuckGo.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string'}, 'max_results': {'type': 'number'}}, 'required': ['query']}}}], tool_choice=None, run_tools=True, show_tool_calls=True, tool_call_limit=None, functions={'duckduckgo_search': Function(name='duckduckgo_search', description='Use this function to search DuckDuckGo for a query.\\n\\nArgs:\\n    query(str): The query to search for.\\n    max_results (optional, default=5): The maximum number of results to return.\\n\\nReturns:\\n    The result from DuckDuckGo.', parameters={'type': 'object', 'properties': {'query': {'type': 'string'}, 'max_results': {'type': 'number'}}, 'required': ['query']}, strict=None, entrypoint=ValidateCallWrapper(<bound method DuckDuckGo.duckduckgo_search of <DuckDuckGo name=duckduckgo functions=['duckduckgo_search', 'duckduckgo_news']>>), sanitize_arguments=True, show_result=False, stop_after_tool_call=False, pre_hook=None, post_hook=None), 'duckduckgo_news': Function(name='duckduckgo_news', description='Use this function to get the latest news from DuckDuckGo.\\n\\nArgs:\\n    query(str): The query to search for.\\n    max_results (optional, default=5): The maximum number of results to return.\\n\\nReturns:\\n    The latest news from DuckDuckGo.', parameters={'type': 'object', 'properties': {'query': {'type': 'string'}, 'max_results': {'type': 'number'}}, 'required': ['query']}, strict=None, entrypoint=ValidateCallWrapper(<bound method DuckDuckGo.duckduckgo_news of <DuckDuckGo name=duckduckgo functions=['duckduckgo_search', 'duckduckgo_news']>>), sanitize_arguments=True, show_result=False, stop_after_tool_call=False, pre_hook=None, post_hook=None)}, function_call_stack=[FunctionCall(function=Function(name='duckduckgo_search', description='Use this function to search DuckDuckGo for a query.\\n\\nArgs:\\n    query(str): The query to search for.\\n    max_results (optional, default=5): The maximum number of results to return.\\n\\nReturns:\\n    The result from DuckDuckGo.', parameters={'type': 'object', 'properties': {'query': {'type': 'string'}, 'max_results': {'type': 'number'}}, 'required': ['query']}, strict=None, entrypoint=ValidateCallWrapper(<bound method DuckDuckGo.duckduckgo_search of <DuckDuckGo name=duckduckgo functions=['duckduckgo_search', 'duckduckgo_news']>>), sanitize_arguments=True, show_result=False, stop_after_tool_call=False, pre_hook=None, post_hook=None), arguments={'query': 'OpenAI Sora', 'max_results': 1}, result='[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', call_id='call_VlPc6PucIGtqgAG8yuNCcGbg', error=None)], system_prompt=None, instructions=None, session_id='10643627-79c3-4819-83bf-5c08c768568a', structured_outputs=False, supports_structured_outputs=True, store=None, metadata=None, frequency_penalty=None, logit_bias=None, logprobs=None, top_logprobs=None, max_tokens=None, max_completion_tokens=None, modalities=None, audio=None, presence_penalty=None, seed=None, stop=None, temperature=None, user=None, top_p=None, extra_headers=None, extra_query=None, request_params=None, api_key='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', organization=None, base_url=None, timeout=None, max_retries=None, default_headers=None, default_query=None, http_client=None, client_params=None, client=None, async_client=None), name='Web Agent', agent_id='f651e909-8ac6-42f8-a9a9-3240c59367f3', introduction=None, images=None, videos=None, audio=None, agent_data=None, user_id=None, user_data=None, session_id='10643627-79c3-4819-83bf-5c08c768568a', session_name=None, session_state={}, session_data=None, memory=AgentMemory(runs=[AgentRun(message=Message(role='user', content='Tell me about OpenAI Sora?', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890214), messages=None, response=RunResponse(content='\\nRunning:\\n - duckduckgo_search(query=OpenAI Sora, max_results=1)\\n\\nI couldn\\'t find specific information about \"OpenAI Sora\" in the search results. It\\'s possible that \"OpenAI Sora\" is a newer project or a less publicized topic not widely covered by content available up to my training data or in search results. \\n\\nIf \"OpenAI Sora\" is a specific product, feature, or concept released under a different name, further details could be explored based on additional context or sources.\\n\\nIf you have more specific information or context about \"OpenAI Sora,\" please share it, and I can help search or clarify further!', content_type='str', event='RunResponse', messages=[Message(role='system', content='## Instructions\\n- Always include sources\\n- Use markdown to format your answers.', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='user', content='Tell me about OpenAI Sora?', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='assistant', content=None, name=None, tool_call_id=None, tool_calls=[{'id': 'call_VlPc6PucIGtqgAG8yuNCcGbg', 'type': 'function', 'function': {'name': 'duckduckgo_search', 'arguments': '{\"query\":\"OpenAI Sora\",\"max_results\":1}'}}], audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 1.2400782000040635, 'time_to_first_token': 1.2255194999743253, 'input_tokens': 189, 'output_tokens': 25, 'prompt_tokens': 189, 'completion_tokens': 25, 'total_tokens': 214, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890209), Message(role='tool', content='[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', name=None, tool_call_id='call_VlPc6PucIGtqgAG8yuNCcGbg', tool_calls=None, audio=None, images=None, videos=None, tool_name='duckduckgo_search', tool_args={'query': 'OpenAI Sora', 'max_results': 1}, tool_call_error=False, stop_after_tool_call=False, metrics={'time': 2.1530698999995366}, references=None, created_at=1735890211), Message(role='assistant', content='I couldn\\'t find specific information about \"OpenAI Sora\" in the search results. It\\'s possible that \"OpenAI Sora\" is a newer project or a less publicized topic not widely covered by content available up to my training data or in search results. \\n\\nIf \"OpenAI Sora\" is a specific product, feature, or concept released under a different name, further details could be explored based on additional context or sources.\\n\\nIf you have more specific information or context about \"OpenAI Sora,\" please share it, and I can help search or clarify further!', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 3.000474900007248, 'time_to_first_token': 1.5130166000453755, 'input_tokens': 1375, 'output_tokens': 116, 'prompt_tokens': 1375, 'completion_tokens': 116, 'total_tokens': 1491, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890214)], metrics=defaultdict(<class 'list'>, {'time': [1.2400782000040635, 3.000474900007248], 'time_to_first_token': [1.2255194999743253, 1.5130166000453755], 'input_tokens': [189, 1375], 'output_tokens': [25, 116], 'prompt_tokens': [189, 1375], 'completion_tokens': [25, 116], 'total_tokens': [214, 1491], 'prompt_tokens_details': [{'audio_tokens': 0, 'cached_tokens': 0}, {'audio_tokens': 0, 'cached_tokens': 0}], 'completion_tokens_details': [{'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}]}), model='gpt-4o', run_id='2728eaa3-6412-49d9-83a2-5b0a91c0ea17', agent_id='f651e909-8ac6-42f8-a9a9-3240c59367f3', session_id='10643627-79c3-4819-83bf-5c08c768568a', workflow_id=None, tools=[{'content': '[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', 'tool_call_id': 'call_VlPc6PucIGtqgAG8yuNCcGbg', 'tool_name': 'duckduckgo_search', 'tool_args': {'query': 'OpenAI Sora', 'max_results': 1}, 'tool_call_error': False, 'metrics': {'time': 2.1530698999995366}, 'created_at': 1735890211}], images=None, videos=None, audio=None, response_audio=None, extra_data=None, created_at=1735890205))], messages=[Message(role='system', content='## Instructions\\n- Always include sources\\n- Use markdown to format your answers.', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='user', content='Tell me about OpenAI Sora?', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='assistant', content=None, name=None, tool_call_id=None, tool_calls=[{'id': 'call_VlPc6PucIGtqgAG8yuNCcGbg', 'type': 'function', 'function': {'name': 'duckduckgo_search', 'arguments': '{\"query\":\"OpenAI Sora\",\"max_results\":1}'}}], audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 1.2400782000040635, 'time_to_first_token': 1.2255194999743253, 'input_tokens': 189, 'output_tokens': 25, 'prompt_tokens': 189, 'completion_tokens': 25, 'total_tokens': 214, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890209), Message(role='tool', content='[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', name=None, tool_call_id='call_VlPc6PucIGtqgAG8yuNCcGbg', tool_calls=None, audio=None, images=None, videos=None, tool_name='duckduckgo_search', tool_args={'query': 'OpenAI Sora', 'max_results': 1}, tool_call_error=False, stop_after_tool_call=False, metrics={'time': 2.1530698999995366}, references=None, created_at=1735890211), Message(role='assistant', content='I couldn\\'t find specific information about \"OpenAI Sora\" in the search results. It\\'s possible that \"OpenAI Sora\" is a newer project or a less publicized topic not widely covered by content available up to my training data or in search results. \\n\\nIf \"OpenAI Sora\" is a specific product, feature, or concept released under a different name, further details could be explored based on additional context or sources.\\n\\nIf you have more specific information or context about \"OpenAI Sora,\" please share it, and I can help search or clarify further!', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 3.000474900007248, 'time_to_first_token': 1.5130166000453755, 'input_tokens': 1375, 'output_tokens': 116, 'prompt_tokens': 1375, 'completion_tokens': 116, 'total_tokens': 1491, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890214)], update_system_message_on_change=False, create_session_summary=False, update_session_summary_after_run=True, summary=None, summarizer=None, create_user_memories=False, update_user_memories_after_run=True, db=None, user_id=None, retrieval=<MemoryRetrieval.last_n: 'last_n'>, memories=None, num_memories=None, classifier=None, manager=None, updating_memory=False), add_history_to_messages=False, num_history_responses=3, knowledge=None, add_references=False, retriever=None, references_format='json', storage=None, tools=[<DuckDuckGo name=duckduckgo functions=['duckduckgo_search', 'duckduckgo_news']>], show_tool_calls=True, tool_call_limit=None, tool_choice=None, context=None, add_context=False, resolve_context=True, reasoning=False, reasoning_model=None, reasoning_agent=None, reasoning_min_steps=1, reasoning_max_steps=10, read_chat_history=False, search_knowledge=True, update_knowledge=False, read_tool_call_history=False, add_messages=None, system_prompt=None, system_prompt_template=None, use_default_system_message=True, system_message_role='system', description=None, task=None, instructions=['Always include sources'], guidelines=None, expected_output=None, additional_context=None, prevent_hallucinations=False, prevent_prompt_leakage=False, limit_tool_access=False, markdown=True, add_name_to_instructions=False, add_datetime_to_instructions=False, user_prompt=None, user_prompt_template=None, use_default_user_message=True, user_message_role='user', response_model=None, parse_response=True, structured_outputs=False, save_response_to_file=None, team=None, role=None, respond_directly=False, add_transfer_instructions=True, team_response_separator='\\n', debug_mode=False, monitoring=False, telemetry=True, run_id='2728eaa3-6412-49d9-83a2-5b0a91c0ea17', run_input='Tell me about OpenAI Sora?', run_response=RunResponse(content='\\nRunning:\\n - duckduckgo_search(query=OpenAI Sora, max_results=1)\\n\\nI couldn\\'t find specific information about \"OpenAI Sora\" in the search results. It\\'s possible that \"OpenAI Sora\" is a newer project or a less publicized topic not widely covered by content available up to my training data or in search results. \\n\\nIf \"OpenAI Sora\" is a specific product, feature, or concept released under a different name, further details could be explored based on additional context or sources.\\n\\nIf you have more specific information or context about \"OpenAI Sora,\" please share it, and I can help search or clarify further!', content_type='str', event='RunResponse', messages=[Message(role='system', content='## Instructions\\n- Always include sources\\n- Use markdown to format your answers.', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='user', content='Tell me about OpenAI Sora?', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={}, references=None, created_at=1735890208), Message(role='assistant', content=None, name=None, tool_call_id=None, tool_calls=[{'id': 'call_VlPc6PucIGtqgAG8yuNCcGbg', 'type': 'function', 'function': {'name': 'duckduckgo_search', 'arguments': '{\"query\":\"OpenAI Sora\",\"max_results\":1}'}}], audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 1.2400782000040635, 'time_to_first_token': 1.2255194999743253, 'input_tokens': 189, 'output_tokens': 25, 'prompt_tokens': 189, 'completion_tokens': 25, 'total_tokens': 214, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890209), Message(role='tool', content='[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', name=None, tool_call_id='call_VlPc6PucIGtqgAG8yuNCcGbg', tool_calls=None, audio=None, images=None, videos=None, tool_name='duckduckgo_search', tool_args={'query': 'OpenAI Sora', 'max_results': 1}, tool_call_error=False, stop_after_tool_call=False, metrics={'time': 2.1530698999995366}, references=None, created_at=1735890211), Message(role='assistant', content='I couldn\\'t find specific information about \"OpenAI Sora\" in the search results. It\\'s possible that \"OpenAI Sora\" is a newer project or a less publicized topic not widely covered by content available up to my training data or in search results. \\n\\nIf \"OpenAI Sora\" is a specific product, feature, or concept released under a different name, further details could be explored based on additional context or sources.\\n\\nIf you have more specific information or context about \"OpenAI Sora,\" please share it, and I can help search or clarify further!', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, metrics={'time': 3.000474900007248, 'time_to_first_token': 1.5130166000453755, 'input_tokens': 1375, 'output_tokens': 116, 'prompt_tokens': 1375, 'completion_tokens': 116, 'total_tokens': 1491, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}}, references=None, created_at=1735890214)], metrics=defaultdict(<class 'list'>, {'time': [1.2400782000040635, 3.000474900007248], 'time_to_first_token': [1.2255194999743253, 1.5130166000453755], 'input_tokens': [189, 1375], 'output_tokens': [25, 116], 'prompt_tokens': [189, 1375], 'completion_tokens': [25, 116], 'total_tokens': [214, 1491], 'prompt_tokens_details': [{'audio_tokens': 0, 'cached_tokens': 0}, {'audio_tokens': 0, 'cached_tokens': 0}], 'completion_tokens_details': [{'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}]}), model='gpt-4o', run_id='2728eaa3-6412-49d9-83a2-5b0a91c0ea17', agent_id='f651e909-8ac6-42f8-a9a9-3240c59367f3', session_id='10643627-79c3-4819-83bf-5c08c768568a', workflow_id=None, tools=[{'content': '[\\n  {\\n    \"title\": \"Curious About OpenAI ChatGPT? - OpenAI ChatGPT vs Grammarly\",\\n    \"href\": \"https://www.bing.com/aclick?ld=e8lHZG2LLKXCu5rqkjQ7qp1jVUCUyhlK1ayHxpCuJxm-LahQ-vBzEc0q088yubdX1MtThdIhenHrn9-i1UGFP034jF2rjQ-V4q80im0ch7NNQ9gPyFVa9e9rLFArvqjn_RkrjfzvXcVAkmNotSq3ZVqkhCsccKC_c1ZQqkHLm0tEAHUzJ-jm_wm2s2OO2169G6fdK6AQ&u=aHR0cHMlM2ElMmYlMmZhZC5kb3VibGVjbGljay5uZXQlMmZzZWFyY2hhZHMlMmZsaW5rJTJmY2xpY2slM2ZsaWQlM2Q0MzcwMDA3ODk4MTkyOTk4MCUyNmRzX3Nfa3dnaWQlM2Q1ODcwMDAwODYzNDQwMDI5NyUyNmRzX2FfY2lkJTNkNTkzMjc0MTQ2JTI2ZHNfYV9jYWlkJTNkMjA4NjQxODQxMzIlMjZkc19hX2FnaWQlM2QxNTY5NzYyOTkzMTclMjZkc19hX2xpZCUzZGt3ZC0zNTA1NjcxOTQ0ODclMjYlMjZkc19lX2FkaWQlM2Q4MTE1NzgyNTk4MzEzMSUyNmRzX2VfdGFyZ2V0X2lkJTNka3dkLTgxMTU4MDE1NzUxMTY4JTNhbG9jLTkwJTI2JTI2ZHNfZV9uZXR3b3JrJTNkcyUyNmRzX3VybF92JTNkMiUyNmRzX2Rlc3RfdXJsJTNkaHR0cHMlM2ElMmYlMmZ3d3cuZ3JhbW1hcmx5LmNvbSUyZmElMmZjaGF0Z3B0JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkY3BjJTI2dXRtX2NhbXBhaWduJTNkNjI3MjE3NzEyJTI2dXRtX2NvbnRlbnQlM2Q4MTE1NzgyNTk4MzEzMSUyNnV0bV90ZXJtJTNkb3BlbmFpJTI2a2V5d29yZGlkJTNkODExNTgwMTU3NTExNjglMjZ0YXJnZXRpZCUzZGt3ZC04MTE1ODAxNTc1MTE2OCUzYWxvYy05MCUyNmFkZ3JvdXAlM2QxMjk4NTI0MzUwNDUyMzIzJTI2ZGV2aWNlJTNkYyUyNm1hdGNodHlwZSUzZHAlMjZuZXR3b3JrJTNkcyUyNmV4dGVuc2lvbiUzZCUyNmNsaWNraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNmdjbGlkJTNkYzBlNzRiNzliZjk2MTRiODY2ZDYyYzBlMmFiNWIwMjUlMjZnY2xzcmMlM2QzcC5kcyUyNiUyNm1zY2xraWQlM2RjMGU3NGI3OWJmOTYxNGI4NjZkNjJjMGUyYWI1YjAyNSUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZDIwMjMlMjUyMC0lMjUyMFNlYXJjaCUyNTIwLSUyNTIwVDQlMjUyMC0lMjUyMENoYXRHUFQlMjZ1dG1fdGVybSUzZG9wZW5haSUyNnV0bV9jb250ZW50JTNkT3BlbkFJ&rlid=c0e74b79bf9614b866d62c0e2ab5b025\",\\n    \"body\": \"Compose, brainstorm, rewrite, and reply using context-specific AI writing prompts. Improve your writing and communication across 500,000+ apps and websites.\"\\n  }\\n]', 'tool_call_id': 'call_VlPc6PucIGtqgAG8yuNCcGbg', 'tool_name': 'duckduckgo_search', 'tool_args': {'query': 'OpenAI Sora', 'max_results': 1}, 'tool_call_error': False, 'metrics': {'time': 2.1530698999995366}, 'created_at': 1735890211}], images=None, videos=None, audio=None, response_audio=None, extra_data=None, created_at=1735890205), stream=True, stream_intermediate_steps=False)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_agent.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import openai\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with your actual API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "MODEL = \"models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\daefdd1f6ae931839bce4d0f3db0a1a4265cd50f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "        \n",
    "    def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n",
    "        # Tokenize text\n",
    "        encoded_text = self.tokenizer(text, return_tensors='pt')\n",
    "        \n",
    "        # Get model output\n",
    "        output = self.model(**encoded_text)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        \n",
    "        # Map scores to labels\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        \n",
    "        # Get highest scoring sentiment\n",
    "        max_score_idx = np.argmax(scores)\n",
    "        sentiment = labels[max_score_idx]\n",
    "        \n",
    "        # Convert to -1 to 1 scale\n",
    "        if sentiment == 'Neutral':\n",
    "            normalized_score = 0.0\n",
    "        elif sentiment == 'Positive':\n",
    "            normalized_score = scores[2]\n",
    "        else:\n",
    "            normalized_score = -scores[0]\n",
    "            \n",
    "        return {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"score\": float(normalized_score),\n",
    "            \"confidence_scores\": {\n",
    "                \"negative\": float(scores[0]),\n",
    "                \"neutral\": float(scores[1]),\n",
    "                \"positive\": float(scores[2])\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI function definition\n",
    "sentiment_function = {\n",
    "    \"name\": \"analyze_sentiment\",\n",
    "    \"description\": \"Analyzes the sentiment of input text and returns a sentiment score and label\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The text to analyze for sentiment\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"text\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Test function\n",
    "def test_sentiment_analyzer():\n",
    "    test_texts = [\n",
    "        \"I absolutely love this product! It's amazing!\",\n",
    "        \"This is the worst experience ever.\",\n",
    "        \"The product was ok ok.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Sentiment Analyzer:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = analyzer.analyze_sentiment(text)\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Sentiment: {result['sentiment']}\")\n",
    "        print(f\"Score: {result['score']:.3f}\")\n",
    "        print(f\"Confidence Scores: {result['confidence_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Sentiment Analyzer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: I absolutely love this product! It's amazing!\n",
      "Sentiment: Positive\n",
      "Score: 0.993\n",
      "Confidence Scores: {'negative': 0.002211926970630884, 'neutral': 0.005183185916393995, 'positive': 0.9926048517227173}\n",
      "\n",
      "Text: This is the worst experience ever.\n",
      "Sentiment: Negative\n",
      "Score: -0.977\n",
      "Confidence Scores: {'negative': 0.9765881299972534, 'neutral': 0.019852112978696823, 'positive': 0.003559779142960906}\n",
      "\n",
      "Text: The product was ok ok.\n",
      "Sentiment: Positive\n",
      "Score: 0.874\n",
      "Confidence Scores: {'negative': 0.0075782532803714275, 'neutral': 0.1186322346329689, 'positive': 0.8737894892692566}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run test\n",
    "if __name__ == \"__main__\":\n",
    "    test_sentiment_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment,\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: normalized_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m             }\n\u001b[0;32m     41\u001b[0m         }\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Initialize analyzer\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Define the function schema following OpenAI's guidelines\u001b[39;00m\n\u001b[0;32m     47\u001b[0m sentiment_tool \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     }\n\u001b[0;32m     65\u001b[0m }\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mSentimentAnalyzer.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels--cardiffnlp--twitter-roberta-base-sentiment\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msnapshots\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m9ed63a080cf4896dafc6f295ddd94c44e1b7104f\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL)\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:939\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2202\u001b[0m     )\n\u001b[0;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.MODEL = \"models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.MODEL)\n",
    "        \n",
    "    def analyze(self, text: str) -> dict:\n",
    "        # Tokenize text\n",
    "        encoded_text = self.tokenizer(text, return_tensors='pt')\n",
    "        \n",
    "        # Get model output\n",
    "        output = self.model(**encoded_text)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        \n",
    "        # Map scores to labels\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        \n",
    "        # Get highest scoring sentiment\n",
    "        max_score_idx = np.argmax(scores)\n",
    "        sentiment = labels[max_score_idx]\n",
    "        \n",
    "        # Convert to -1 to 1 scale\n",
    "        normalized_score = float(scores[2] - scores[0])\n",
    "        \n",
    "        return {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"score\": normalized_score,\n",
    "            \"confidence_scores\": {\n",
    "                \"negative\": float(scores[0]),\n",
    "                \"neutral\": float(scores[1]),\n",
    "                \"positive\": float(scores[2])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Define the function schema following OpenAI's guidelines\n",
    "sentiment_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_sentiment\",\n",
    "        \"description\": \"Analyzes the sentiment of input text and returns sentiment scores\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The text to analyze for sentiment\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with OpenAI\n",
    "def get_sentiment_analysis(client: OpenAI, text: str) -> str:\n",
    "    # First API call to get function call\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes sentiment in text.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the sentiment of this text: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[sentiment_tool]\n",
    "    )\n",
    "    \n",
    "    # Get the function call\n",
    "    tool_call = completion.choices[0].message.tool_calls[0]\n",
    "    \n",
    "    # Parse arguments and run analysis\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = analyzer.analyze(args[\"text\"])\n",
    "    \n",
    "    # Append the result and get final response\n",
    "    messages.append(completion.choices[0].message)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": json.dumps(result)\n",
    "    })\n",
    "    \n",
    "    final_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[sentiment_tool]\n",
    "    )\n",
    "    \n",
    "    return final_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Sentiment Analyzer:\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing: I absolutely love this product! It's amazing!\n",
      "Response: The sentiment of the text \"I absolutely love this product! It's amazing!\" is highly positive, with a score of 0.990. The system is 99.2% confident about this classification.\n",
      "\n",
      "Analyzing: This is the worst experience ever.\n",
      "Response: The sentiment of the text is Negative with a high confidence score (97.65%). This indicates that the text overwhelmingly expresses a negative sentiment.\n",
      "\n",
      "Analyzing: The weather is quite normal today.\n",
      "Response: The sentiment of the text \"The weather is quite normal today.\" is positive with a high confidence of approximately 85.57%. The sentiment score is 0.851 indicating a strong positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test function\n",
    "def test_sentiment_analyzer():\n",
    "    client = OpenAI()  # Make sure to set your API key\n",
    "    \n",
    "    test_texts = [\n",
    "        \"I absolutely love this product! It's amazing!\",\n",
    "        \"This is the worst experience ever.\",\n",
    "        \"The weather is quite normal today.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Sentiment Analyzer:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(f\"\\nAnalyzing: {text}\")\n",
    "        response = get_sentiment_analysis(client, text)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sentiment_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoImageProcessor,\n",
    "    M2M100Tokenizer, \n",
    "    M2M100ForConditionalGeneration\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "from typing import Dict, Any, Union, List\n",
    "import json\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize sentiment analysis\n",
    "        self.sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "        \n",
    "        # Initialize image classification\n",
    "        self.image_model_name = \"microsoft/resnet-50\"\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(self.image_model_name)\n",
    "        self.image_model = AutoModelForImageClassification.from_pretrained(self.image_model_name)\n",
    "        \n",
    "        # Initialize translation\n",
    "        self.translation_model_name = \"facebook/m2m100_418M\"\n",
    "        self.translation_tokenizer = M2M100Tokenizer.from_pretrained(self.translation_model_name)\n",
    "        self.translation_model = M2M100ForConditionalGeneration.from_pretrained(self.translation_model_name)\n",
    "        \n",
    "        # Supported languages for translation\n",
    "        self.supported_languages = {\n",
    "            \"en\": \"English\", \"es\": \"Spanish\", \"fr\": \"French\",\n",
    "            \"de\": \"German\", \"it\": \"Italian\", \"pt\": \"Portuguese\",\n",
    "            \"nl\": \"Dutch\", \"ru\": \"Russian\", \"zh\": \"Chinese\",\n",
    "            \"ja\": \"Japanese\"\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n",
    "        encoded_text = self.sentiment_tokenizer(text, return_tensors='pt', truncation=True)\n",
    "        output = self.sentiment_model(**encoded_text)\n",
    "        scores = torch.nn.functional.softmax(output.logits[0], dim=0)\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        return {\n",
    "            \"sentiment\": labels[scores.argmax().item()],\n",
    "            \"confidence_scores\": {\n",
    "                label: score.item()\n",
    "                for label, score in zip(labels, scores)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def classify_image(self, image_data: Union[str, bytes]) -> Dict[str, Any]:\n",
    "        # Handle base64 encoded images\n",
    "        if isinstance(image_data, str):\n",
    "            image_data = base64.b64decode(image_data)\n",
    "            \n",
    "        # Open image\n",
    "        image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Process image\n",
    "        inputs = self.image_processor(image, return_tensors=\"pt\")\n",
    "        outputs = self.image_model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits[0], dim=0)\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_5_probs, top_5_indices = torch.topk(probs, 5)\n",
    "        \n",
    "        return {\n",
    "            \"classifications\": [\n",
    "                {\n",
    "                    \"label\": self.image_model.config.id2label[idx.item()],\n",
    "                    \"confidence\": prob.item()\n",
    "                }\n",
    "                for prob, idx in zip(top_5_probs, top_5_indices)\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def translate_text(self, text: str, source_lang: str, target_lang: str) -> Dict[str, Any]:\n",
    "        if source_lang not in self.supported_languages or target_lang not in self.supported_languages:\n",
    "            raise ValueError(\"Unsupported language code\")\n",
    "            \n",
    "        self.translation_tokenizer.src_lang = source_lang\n",
    "        encoded_text = self.translation_tokenizer(text, return_tensors=\"pt\")\n",
    "        generated_tokens = self.translation_model.generate(\n",
    "            **encoded_text,\n",
    "            forced_bos_token_id=self.translation_tokenizer.get_lang_id(target_lang)\n",
    "        )\n",
    "        \n",
    "        translation = self.translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return {\n",
    "            \"translated_text\": translation,\n",
    "            \"source_language\": self.supported_languages[source_lang],\n",
    "            \"target_language\": self.supported_languages[target_lang]\n",
    "        }\n",
    "    \n",
    "    def analyze_content(self, \n",
    "                       content_type: str,\n",
    "                       content: Union[str, bytes],\n",
    "                       source_lang: str = None,\n",
    "                       target_lang: str = None) -> Dict[str, Any]:\n",
    "        results = {\"content_type\": content_type}\n",
    "        \n",
    "        if content_type == \"text\":\n",
    "            # Perform sentiment analysis\n",
    "            sentiment_results = self.analyze_sentiment(content)\n",
    "            results[\"sentiment_analysis\"] = sentiment_results\n",
    "            \n",
    "            # Perform translation if languages are specified\n",
    "            if source_lang and target_lang:\n",
    "                translation_results = self.translate_text(content, source_lang, target_lang)\n",
    "                results[\"translation\"] = translation_results\n",
    "                \n",
    "        elif content_type == \"image\":\n",
    "            # Perform image classification\n",
    "            classification_results = self.classify_image(content)\n",
    "            results[\"image_analysis\"] = classification_results\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_content\",\n",
    "        \"description\": \"Analyzes text or image content providing sentiment analysis, image classification, and translation capabilities\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"content_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"text\", \"image\"],\n",
    "                    \"description\": \"Type of content to analyze\"\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The content to analyze. For images, should be base64-encoded string\"\n",
    "                },\n",
    "                \"source_lang\": {\n",
    "                    \"type\": [\"string\", \"null\"],  # Make it nullable\n",
    "                    \"enum\": [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"nl\", \"ru\", \"zh\", \"ja\"],\n",
    "                    \"description\": \"Source language code for translation\"\n",
    "                },\n",
    "                \"target_lang\": {\n",
    "                    \"type\": [\"string\", \"null\"],  # Make it nullable\n",
    "                    \"enum\": [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"nl\", \"ru\", \"zh\", \"ja\"],\n",
    "                    \"description\": \"Target language code for translation\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"content_type\", \"content\", \"source_lang\", \"target_lang\"],  # All properties must be required\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_analysis(client: OpenAI, query: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that can analyze text and images, providing sentiment analysis, classification, and translation.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[multimodal_tool]\n",
    "    )\n",
    "    \n",
    "    # Get the function call\n",
    "    tool_call = completion.choices[0].message.tool_calls[0]\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = MultiModalAnalyzer()\n",
    "    \n",
    "    # Parse arguments and run analysis\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    # Ensure all required parameters are present\n",
    "    if \"source_lang\" not in args:\n",
    "        args[\"source_lang\"] = None\n",
    "    if \"target_lang\" not in args:\n",
    "        args[\"target_lang\"] = None\n",
    "        \n",
    "    result = analyzer.analyze_content(**args)\n",
    "    \n",
    "    # Append the result and get final response\n",
    "    messages.append(completion.choices[0].message)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": json.dumps(result)\n",
    "    })\n",
    "    \n",
    "    final_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[multimodal_tool]\n",
    "    )\n",
    "    \n",
    "    return final_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "  \"\"\"\n",
    "  Loads an image from the given path and converts it to a Base64 string.\n",
    "\n",
    "  Args:\n",
    "    image_path: The path to the image file.\n",
    "\n",
    "  Returns:\n",
    "    The Base64 encoded string of the image.\n",
    "  \"\"\"\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    image_data = image_file.read()\n",
    "  base64_encoded_image = base64.b64encode(image_data)\n",
    "  return base64_encoded_image.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_path = \"MainAfter.jpg\" \n",
    "base64_string = image_to_base64(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Image Analysis:\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 83032 tokens (82855 in the messages, 177 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(process_content_analysis(client, image_query))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mtest_multimodal_analyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtest_multimodal_analyzer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m image_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyze this image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase64_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting Image Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprocess_content_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_query\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mprocess_content_analysis\u001b[1;34m(client, query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_content_analysis\u001b[39m(client: OpenAI, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that can analyze text and images, providing sentiment analysis, classification, and translation.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query}\n\u001b[0;32m      5\u001b[0m     ]\n\u001b[1;32m----> 7\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmultimodal_tool\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get the function call\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     tool_call \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1277\u001b[0m     )\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ujjaval\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\openai\\_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1068\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 83032 tokens (82855 in the messages, 177 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "def test_multimodal_analyzer():\n",
    "    client = OpenAI(api_key=\"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\")\n",
    "    \n",
    "#     # Test text analysis\n",
    "#     text_query = \"Analyze this text: 'I'm really excited about this new project!' and translate it to Spanish\"\n",
    "#     print(\"\\nTesting Text Analysis:\")\n",
    "#     print(process_content_analysis(client, text_query))\n",
    "    \n",
    "    # Test image analysis\n",
    "    image_query = f\"Analyze this image: {base64_string}\"\n",
    "    print(\"\\nTesting Image Analysis:\")\n",
    "    print(process_content_analysis(client, image_query))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_multimodal_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForImageClassification,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "from typing import Dict, Any, Union\n",
    "import json\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize image classification\n",
    "        self.image_model_name = \"microsoft/resnet-50\"\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(self.image_model_name)\n",
    "        self.image_model = AutoModelForImageClassification.from_pretrained(self.image_model_name)\n",
    "    \n",
    "    def classify_image(self, image_url: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            # Download image from URL\n",
    "            response = requests.get(image_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            \n",
    "            # Process image\n",
    "            inputs = self.image_processor(image, return_tensors=\"pt\")\n",
    "            outputs = self.image_model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits[0], dim=0)\n",
    "            \n",
    "            # Get top 5 predictions\n",
    "            top_5_probs, top_5_indices = torch.topk(probs, 5)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"classifications\": [\n",
    "                    {\n",
    "                        \"label\": self.image_model.config.id2label[idx.item()],\n",
    "                        \"confidence\": float(prob.item())  # Convert to float for JSON serialization\n",
    "                    }\n",
    "                    for prob, idx in zip(top_5_probs, top_5_indices)\n",
    "                ]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OpenAI function schema\n",
    "image_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_image\",\n",
    "        \"description\": \"Analyzes an image and provides classification results\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"image_url\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"URL of the image to analyze\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"image_url\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with OpenAI\n",
    "def process_image_analysis(client, image_url: str) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a helpful assistant that can analyze images and provide detailed descriptions of their content.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Please analyze this image: {image_url}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[image_tool]\n",
    "    )\n",
    "    \n",
    "    # Get the function call\n",
    "    tool_call = completion.choices[0].message.tool_calls[0]\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = ImageAnalyzer()\n",
    "    \n",
    "    # Parse arguments and run analysis\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = analyzer.classify_image(args[\"image_url\"])\n",
    "    \n",
    "    # Append the result and get final response\n",
    "    messages.append(completion.choices[0].message)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": json.dumps(result)\n",
    "    })\n",
    "    \n",
    "    final_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=[image_tool]\n",
    "    )\n",
    "    \n",
    "    return final_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Image Analysis:\n",
      "The image appears to represent a coastal or lakeshore scene, possibly depicting a volcanic landscape. These conclusions are made with varying degrees of confidence. The highest confidence is attached to the categories \"seashore, coast, seacoast, sea-coast\" (confidence: ~42.9%) and \"lakeside, lakeshore\" (confidence: ~29.8%). Smaller probabilities suggest the image may show a volcano (confidence: ~7.6%), a lighthouse (confidence: ~4.2%), or utilize a spotlight or spot focus (confidence: ~2.5%). \n",
      "\n",
      "Please note, these predictions are generated by an AI and there might be some inaccuracies.\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "def test_image_analyzer():\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=\"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\")  # Make sure your API key is set\n",
    "    \n",
    "    # Test with a sample image URL\n",
    "    test_image_url = \"https://cdn.pixabay.com/photo/2015/04/23/22/00/new-year-background-736885_1280.jpg\"\n",
    "    print(\"\\nTesting Image Analysis:\")\n",
    "    try:\n",
    "        result = process_image_analysis(client, test_image_url)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_image_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ImageAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'classifications': [{'label': 'seashore, coast, seacoast, sea-coast',\n",
       "   'confidence': 0.42855843901634216},\n",
       "  {'label': 'lakeside, lakeshore', 'confidence': 0.29845213890075684},\n",
       "  {'label': 'volcano', 'confidence': 0.07633821666240692},\n",
       "  {'label': 'beacon, lighthouse, beacon light, pharos',\n",
       "   'confidence': 0.041634682565927505},\n",
       "  {'label': 'spotlight, spot', 'confidence': 0.02498311921954155}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.classify_image(image_url=\"https://cdn.pixabay.com/photo/2015/04/23/22/00/new-year-background-736885_1280.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Approch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sentiment analysis model...\n",
      "Sentiment analysis model downloaded to: models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\9ed63a080cf4896dafc6f295ddd94c44e1b7104f\n",
      "Downloading image classification model...\n",
      "Image classification model downloaded to: models\\models--microsoft--resnet-50\\snapshots\\34c2154c194f829b11125337b98c8f5f9965ff19\n",
      "Downloading translation model...\n",
      "Translation model downloaded to: models\\models--facebook--m2m100_418M\\snapshots\\55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636\n",
      "All models downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    M2M100Tokenizer,\n",
    "    M2M100ForConditionalGeneration\n",
    ")\n",
    "import os\n",
    "\n",
    "# Directory to store the models\n",
    "cache_dir = \"models\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Download sentiment analysis model\n",
    "print(\"Downloading sentiment analysis model...\")\n",
    "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name, cache_dir=cache_dir)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, cache_dir=cache_dir)\n",
    "\n",
    "# Locate the snapshot directory for the sentiment model\n",
    "sentiment_snapshot_dir = os.path.join(cache_dir, f\"models--{sentiment_model_name.replace('/', '--')}\", \"snapshots\")\n",
    "sentiment_snapshot_hash = os.listdir(sentiment_snapshot_dir)[0]  # Get the first snapshot hash\n",
    "sentiment_model_path = os.path.join(sentiment_snapshot_dir, sentiment_snapshot_hash)\n",
    "print(f\"Sentiment analysis model downloaded to: {sentiment_model_path}\")\n",
    "\n",
    "# Download image classification model\n",
    "print(\"Downloading image classification model...\")\n",
    "image_model_name = \"microsoft/resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(image_model_name, cache_dir=cache_dir)\n",
    "image_model = AutoModelForImageClassification.from_pretrained(image_model_name, cache_dir=cache_dir)\n",
    "\n",
    "# Locate the snapshot directory for the image model\n",
    "image_snapshot_dir = os.path.join(cache_dir, f\"models--{image_model_name.replace('/', '--')}\", \"snapshots\")\n",
    "image_snapshot_hash = os.listdir(image_snapshot_dir)[0]  # Get the first snapshot hash\n",
    "image_model_path = os.path.join(image_snapshot_dir, image_snapshot_hash)\n",
    "print(f\"Image classification model downloaded to: {image_model_path}\")\n",
    "\n",
    "# Download translation model\n",
    "print(\"Downloading translation model...\")\n",
    "translation_model_name = \"facebook/m2m100_418M\"\n",
    "translation_tokenizer = M2M100Tokenizer.from_pretrained(translation_model_name, cache_dir=cache_dir)\n",
    "translation_model = M2M100ForConditionalGeneration.from_pretrained(translation_model_name, cache_dir=cache_dir)\n",
    "\n",
    "# Locate the snapshot directory for the translation model\n",
    "translation_snapshot_dir = os.path.join(cache_dir, f\"models--{translation_model_name.replace('/', '--')}\", \"snapshots\")\n",
    "translation_snapshot_hash = os.listdir(translation_snapshot_dir)[0]  # Get the first snapshot hash\n",
    "translation_model_path = os.path.join(translation_snapshot_dir, translation_snapshot_hash)\n",
    "print(f\"Translation model downloaded to: {translation_model_path}\")\n",
    "\n",
    "print(\"All models downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_path =  \"models\\models--cardiffnlp--twitter-roberta-base-sentiment\\snapshots\\daefdd1f6ae931839bce4d0f3db0a1a4265cd50f\"\n",
    "translation_model_path = \"D:\\\\Working Blue Data\\\\AI Product\\\\AI Embedness\\\\models\\\\models--facebook--m2m100_418M\\\\snapshots\\\\55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636\"\n",
    "image_model_path= \"D:\\\\Working Blue Data\\\\AI Product\\\\AI Embedness\\\\models\\\\models--microsoft--resnet-50\\\\snapshots\\\\34c2154c194f829b11125337b98c8f5f9965ff19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_2', 'score': 0.9883654117584229}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def sentiment_analysis(text, model_path):\n",
    "    # Load tokenizer and model from the local path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Create a sentiment analysis pipeline\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Get the sentiment result\n",
    "    result = sentiment_pipeline(text)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "text = \"I love using Hugging Face models!\"\n",
    "result = sentiment_analysis(text, sentiment_model_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'African chameleon, Chamaeleo chamaeleon', 'score': 0.9919662475585938}, {'label': 'green lizard, Lacerta viridis', 'score': 0.006578190717846155}, {'label': 'American chameleon, anole, Anolis carolinensis', 'score': 0.00023568913456983864}, {'label': 'agama', 'score': 0.00011581629951251671}, {'label': 'banded gecko', 'score': 4.173571869614534e-05}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "def image_classification(image_path, model_path):\n",
    "    # Load the image processor and model from the local path\n",
    "    processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create an image classification pipeline\n",
    "    image_pipeline = pipeline(\"image-classification\", model=model, feature_extractor=processor)\n",
    "    \n",
    "    # Get the image classification result\n",
    "    result = image_pipeline(image)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "image_path = \"MainAfter.jpg\"  # Replace with the path to your image\n",
    "result = image_classification(image_path, image_model_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment vous tes-vous ?\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "\n",
    "def translate_text(text, model_path, src_lang=\"en\", tgt_lang=\"fr\"):\n",
    "    # Load the tokenizer and model from the local path\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_path)\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_path)\n",
    "    \n",
    "    # Set the source language\n",
    "    tokenizer.src_lang = src_lang\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the translation\n",
    "    generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\n",
    "    \n",
    "    # Decode the generated tokens to get the translated text\n",
    "    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated_text[0]\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "translated_text = translate_text(text, translation_model_path, src_lang=\"en\", tgt_lang=\"fr\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_sentiment\",\n",
    "        \"description\": \"Analyze the sentiment of a given text and return the sentiment score and label.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The text to analyze for sentiment.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the input text using the 'cardiffnlp/twitter-roberta-base-sentiment' model.\n",
    "    Returns a sentiment score and label.\n",
    "    \"\"\"\n",
    "    # Load the model and tokenizer from the local path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path)\n",
    "    \n",
    "    # Create a sentiment analysis pipeline\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Get the sentiment result\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    \n",
    "    # Map the label to a sentiment score (-1 to 1)\n",
    "    label_to_score = {\n",
    "        \"LABEL_0\": -1,  # Negative\n",
    "        \"LABEL_1\": 0,   # Neutral\n",
    "        \"LABEL_2\": 1    # Positive\n",
    "    }\n",
    "\n",
    "    sentiment = {\n",
    "        \"LABEL_0\": 'Negative',  # Negative\n",
    "        \"LABEL_1\": 'Neutral',   # Neutral\n",
    "        \"LABEL_2\": 'Positive'    # Positive\n",
    "\n",
    "    }\n",
    "    \n",
    "    sentiment_score = label_to_score.get(result[\"label\"], 0)\n",
    "    sentiment_label = sentiment.get(result[\"label\"],'Neutral')\n",
    "    \n",
    "    return {\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"sentiment_label\": sentiment_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_multimodal_content\",\n",
    "        \"description\": \"Analyze text sentiment, classify or analyse images, and translate text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The text to analyze for sentiment or translate.\"\n",
    "                },\n",
    "                \"image_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The path to the image file for classification.\"\n",
    "                },\n",
    "                \"translate_source_lang\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The source language for translation (e.g., 'en').\"\n",
    "                },\n",
    "                \"translate_target_lang\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The target language for translation (e.g., 'fr').\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    M2M100Tokenizer,\n",
    "    M2M100ForConditionalGeneration\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "def analyze_multimodal_content(text=None, image_path=None, translate_source_lang=\"en\", translate_target_lang=\"fr\"):\n",
    "    \"\"\"\n",
    "    Analyze text sentiment, classify images, and translate text.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    if text:\n",
    "        sentiment_result = analyze_sentiment(text)\n",
    "        results[\"sentiment_analysis\"] = sentiment_result\n",
    "    \n",
    "    # Image Classification\n",
    "    if image_path:\n",
    "        processor = AutoImageProcessor.from_pretrained(image_model_path)\n",
    "        model = AutoModelForImageClassification.from_pretrained(image_model_path)\n",
    "        image = Image.open(image_path)\n",
    "        image_pipeline = pipeline(\"image-classification\", model=model, feature_extractor=processor)\n",
    "        image_result = image_pipeline(image)\n",
    "        results[\"image_classification\"] = image_result\n",
    "    \n",
    "    # Text Translation\n",
    "    if text and translate_source_lang and translate_target_lang:\n",
    "        tokenizer = M2M100Tokenizer.from_pretrained(translation_model_path)\n",
    "        model = M2M100ForConditionalGeneration.from_pretrained(translation_model_path)\n",
    "        tokenizer.src_lang = translate_source_lang\n",
    "        encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "        generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id(translate_target_lang))\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        results[\"translation\"] = translated_text\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [sentiment_tool, multimodal_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"classify  this image : 'images (1).jpg\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessageToolCall(id='call_5iV6dH0sPQnrIgONm4XyYbtt', function=Function(arguments='{\"image_path\":\"images (1).jpg\"}', name='analyze_multimodal_content'), type='function')\n",
      "analyze_multimodal_content\n",
      "{'image_classification': [{'label': 'teddy, teddy bear', 'score': 0.6075677871704102}, {'label': 'mask', 'score': 0.037414368242025375}, {'label': 'toyshop', 'score': 0.02797093242406845}, {'label': 'bib', 'score': 0.014273123815655708}, {'label': 'pencil sharpener', 'score': 0.009215272963047028}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Extract the function call from the response\n",
    "function_call = response.choices[0].message.tool_calls[0]\n",
    "print(function_call)\n",
    "function_name = function_call.function.name\n",
    "print(function_name)\n",
    "function_args = json.loads(function_call.function.arguments)\n",
    "\n",
    "# Execute the function\n",
    "if function_name == \"analyze_sentiment\":\n",
    "    result = analyze_sentiment(**function_args)\n",
    "elif function_name == \"analyze_multimodal_content\":\n",
    "    result = analyze_multimodal_content(**function_args)\n",
    "else:\n",
    "    result = \"Function not found.\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_key=\"AIzaSyAQMU74h93W8Y5pgvlDRASLGGXxO1_sn0o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini tool-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import PIL.Image\n",
    "import time\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ContentTool:\n",
    "    name: str\n",
    "    description: str\n",
    "    supported_types: List[str]\n",
    "    process_func: callable\n",
    "\n",
    "class UnifiedGeminiAgent:\n",
    "    \"\"\"\n",
    "    A unified agent that automatically handles different types of content through tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_id: str = \"gemini-1.5-flash\"):\n",
    "        \"\"\"Initialize the agent with API key and tools.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_id = model_id\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.model = genai.GenerativeModel(self.model_id)\n",
    "        \n",
    "        # Initialize tools\n",
    "        self._initialize_tools()\n",
    "        \n",
    "        # Initialize chat for interactive sessions\n",
    "        self.chat = self.model.start_chat()\n",
    "\n",
    "    def _initialize_tools(self):\n",
    "        \"\"\"Initialize content processing tools.\"\"\"\n",
    "        self.tools = {\n",
    "            'text': ContentTool(\n",
    "                name=\"text_processor\",\n",
    "                description=\"Process text-only content\",\n",
    "                supported_types=['text/plain'],\n",
    "                process_func=self._process_text\n",
    "            ),\n",
    "            'image': ContentTool(\n",
    "                name=\"image_processor\",\n",
    "                description=\"Process image content\",\n",
    "                supported_types=['image/jpeg', 'image/png', 'image/gif'],\n",
    "                process_func=self._process_image\n",
    "            ),\n",
    "            'audio': ContentTool(\n",
    "                name=\"audio_processor\",\n",
    "                description=\"Process audio content\",\n",
    "                supported_types=['audio/mpeg', 'audio/wav', 'audio/x-wav'],\n",
    "                process_func=self._process_audio\n",
    "            ),\n",
    "            'video': ContentTool(\n",
    "                name=\"video_processor\",\n",
    "                description=\"Process video content\",\n",
    "                supported_types=['video/mp4', 'video/mpeg', 'video/quicktime'],\n",
    "                process_func=self._process_video\n",
    "            ),\n",
    "            'document': ContentTool(\n",
    "                name=\"document_processor\",\n",
    "                description=\"Process document content\",\n",
    "                supported_types=['application/pdf'],\n",
    "                process_func=self._process_document\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _get_content_type(self, file_path: Optional[Union[str, Path]] = None) -> str:\n",
    "        \"\"\"Determine content type from file or assume text if no file provided.\"\"\"\n",
    "        if file_path is None:\n",
    "            return 'text/plain'\n",
    "        \n",
    "        mime_type, _ = mimetypes.guess_type(str(file_path))\n",
    "        return mime_type or 'application/octet-stream'\n",
    "\n",
    "    def _get_appropriate_tool(self, content_type: str) -> Optional[ContentTool]:\n",
    "        \"\"\"Get the appropriate tool for the content type.\"\"\"\n",
    "        for tool in self.tools.values():\n",
    "            if content_type in tool.supported_types:\n",
    "                return tool\n",
    "        return None\n",
    "\n",
    "    def _process_text(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Process text-only content.\"\"\"\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "    def _process_image(self, prompt: str, file_path: Union[str, Path], **kwargs) -> str:\n",
    "        \"\"\"Process image content.\"\"\"\n",
    "        image = PIL.Image.open(file_path)\n",
    "        response = self.model.generate_content([prompt, image])\n",
    "        return response.text\n",
    "\n",
    "    def _process_audio(self, prompt: str, file_path: Union[str, Path], **kwargs) -> str:\n",
    "        \"\"\"Process audio content.\"\"\"\n",
    "        audio_file = genai.upload_file(file_path)\n",
    "        response = self.model.generate_content([prompt, audio_file])\n",
    "        return response.text\n",
    "\n",
    "    def _process_video(self, prompt: str, file_path: Union[str, Path], **kwargs) -> str:\n",
    "        \"\"\"Process video content.\"\"\"\n",
    "        video_file = genai.upload_file(file_path)\n",
    "        \n",
    "        # Wait for video processing\n",
    "        while video_file.state.name == \"PROCESSING\":\n",
    "            print(\"Processing video...\")\n",
    "            time.sleep(5)\n",
    "            video_file = genai.get_file(video_file.name)\n",
    "        \n",
    "        response = self.model.generate_content([prompt, video_file])\n",
    "        return response.text\n",
    "\n",
    "    def _process_document(self, prompt: str, file_path: Union[str, Path], **kwargs) -> str:\n",
    "        \"\"\"Process document content.\"\"\"\n",
    "        doc_file = genai.upload_file(file_path)\n",
    "        response = self.model.generate_content([prompt, doc_file])\n",
    "        return response.text\n",
    "\n",
    "    def process(self, \n",
    "                prompt: str, \n",
    "                file_path: Optional[Union[str, Path]] = None, \n",
    "                **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Main processing method that automatically handles different content types.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The prompt or question for the model\n",
    "            file_path (Optional[Union[str, Path]]): Path to the file to process (if any)\n",
    "            **kwargs: Additional arguments for specific tools\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If content type is not supported\n",
    "        \"\"\"\n",
    "        content_type = self._get_content_type(file_path)\n",
    "        tool = self._get_appropriate_tool(content_type)\n",
    "        \n",
    "        if tool is None:\n",
    "            raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "        \n",
    "        # Process the content using the appropriate tool\n",
    "        if file_path is None:\n",
    "            return tool.process_func(prompt, **kwargs)\n",
    "        else:\n",
    "            return tool.process_func(prompt, file_path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_usage():\n",
    "    # Initialize agent\n",
    "    agent = UnifiedGeminiAgent(api_key=\"AIzaSyAQMU74h93W8Y5pgvlDRASLGGXxO1_sn0o\")\n",
    "    \n",
    "    # Process different types of content\n",
    "    try:\n",
    "        # # Text processing\n",
    "        # text_response = agent.process(\n",
    "        #     prompt=\"What is the capital of France?\"\n",
    "        # )\n",
    "        # print(\"Text Response:\", text_response)\n",
    "        \n",
    "        # # Image processing\n",
    "        # image_response = agent.process(\n",
    "        #     prompt=\"provide bounding box cordinates for sun glass\",\n",
    "        #     file_path=\"download.jpg\"\n",
    "        # )\n",
    "        # print(\"Image Response:\", image_response)\n",
    "        \n",
    "        # Audio processing\n",
    "        audio_response = agent.process(\n",
    "            prompt=\"Transcribe this audio\",\n",
    "            file_path=\"Marvel Televisions Daredevil Born Again.mp3\"\n",
    "        )\n",
    "        print(\"Audio Response:\", audio_response)\n",
    "        \n",
    "        # Video processing\n",
    "        # video_response = agent.process(\n",
    "        #     prompt=\"What's happening in this video?\",\n",
    "        #     file_path=\"videoplayback.mp4\"\n",
    "        # )\n",
    "        # print(\"Video Response:\", video_response)\n",
    "        \n",
    "        # Document processing\n",
    "        # doc_response = agent.process(\n",
    "        #     prompt=\"Summarize this document\",\n",
    "        #     file_path=\"Functions - Phidata.pdf\"\n",
    "        # )\n",
    "        # print(\"Document Response:\", doc_response)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Response: Thank you for finding the time.\n",
      "\n",
      "I will admit, it's not entirely unpleasant seeing you again.\n",
      "\n",
      "A lot of time has passed. By the look of it, you've come up in the world. I could say the same about you.\n",
      "\n",
      "The mayor, serves his city.\n",
      "\n",
      "I can see you're not entirely convinced.\n",
      "\n",
      "Can you blame me?\n",
      "\n",
      "I can't shake the feeling that you're gaming the system.\n",
      "\n",
      "Why did you stop being a vigilante?\n",
      "\n",
      "A line was crossed.\n",
      "\n",
      "It's hard to come to terms with the violent nature.\n",
      "\n",
      "Hating the power it has over us.\n",
      "\n",
      "This city can't go unchecked.\n",
      "\n",
      "Sometimes peace needs to be broken.\n",
      "\n",
      "The chaos must reign.\n",
      "\n",
      "Why am I putting the hatchet down?\n",
      "\n",
      "I was raised to believe in grace.\n",
      "\n",
      "But I was also raised to believe in retribution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
